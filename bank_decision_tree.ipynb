{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_read_clean(filename):\n",
    "    dataset_directory = \"D:\\\\Coursework\\\\Capstone\\\\clean_repo\\\\data_load\\\\dataset\"\n",
    "    bank_vs_demo = pd.read_csv(dataset_directory+\"\\\\\"+filename)\n",
    "    # drop year, state, zip\n",
    "    bank_vs_demo = bank_vs_demo.drop(['index','year','state','zip','bank_open','bank_close',\n",
    "                                                                'bank_net'], axis=1)\n",
    "    data_list = bank_vs_demo.drop('ground_truth', axis=1)\n",
    "    label_list = bank_vs_demo['ground_truth'] \n",
    "    #Create train and test data. test_size 0.3 means 30% of data will be test data.\n",
    "    #change dataframe to matrix - data_list and label-list\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(data_list.as_matrix(), label_list.as_matrix(), test_size=0.3, random_state=None )\n",
    "    return train_features, test_features, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_accuracy(test_labels,pred_labels):\n",
    "    # create confusion matrix\n",
    "    matrix = confusion_matrix(test_labels, pred_test)\n",
    "    accuracy = accuracy_score(test_labels, pred_labels)\n",
    "    # print the accuracy score on the test data\n",
    "    print('Accuracy Score :',accuracy)\n",
    "    return matrix,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(pred, ground):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "\n",
    "    args:\n",
    "        pred: numpy.ndarray(bool) -- predictions\n",
    "        ground: numpy.ndarray(bool) -- known ground-truth values\n",
    "    \n",
    "    return : double -- the F1 score of the predictions\n",
    "    \"\"\"\n",
    "    pred = np.array(pred, dtype=bool)\n",
    "    ground = np.array(ground, dtype=bool)\n",
    "\n",
    "    return f1_score(ground,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(train_features,train_labels):\n",
    "    #option1 decision tree classifier model\n",
    "    clf = tree.DecisionTreeClassifier(max_depth = 3)\n",
    "    clf = clf.fit(train_data, train_label)\n",
    "\n",
    "    # create prediction\n",
    "    predicted = clf.predict(test_data)\n",
    "    return clf,predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"<data_set_file_name>\"\n",
    "train_features, test_features, train_labels, test_labels = file_read_clean(filename)\n",
    "train_preds, model = train_evaluate(train_features, train_labels)\n",
    "test_preds = model.predict(test_features)\n",
    "test_f1_score = f1(test_preds, test_labels)\n",
    "matrix,accuracy = confusion_matrix_accuracy(test_labels,test_preds)\n",
    "\n",
    "# create label\n",
    "labels = ['Open','Constant','Close']\n",
    "\n",
    "# display the heatmap of confusion matrix on the test data\n",
    "fig, ax = plt.subplots(figsize=(10, 5)) \n",
    "ax.set_title('Confusion Matrix - Test Data')\n",
    "sns.heatmap(matrix, annot =True, fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Groundtruth')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
