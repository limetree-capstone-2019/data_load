{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Upload of raw data to snowflake using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be connecting to snowflake and uploading data from all the files in a particular folder to the specified table in snowflake. If the table already exists then we will be appending data else will create a new table and add data to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First run all the necessary import statements. \n",
    "Please install snowflake.connector and snowflake.sqlalchemy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from snowflake.sqlalchemy import URL\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the credentials to connect to our isntance of snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials for connecting to snowflake. Migrate to properties file in the future.\n",
    "USER = 'heinzlimetree'\n",
    "ACCOUNT = 'um21928.us-east-1'\n",
    "WAREHOUSE = 'COMPUTE-WH'\n",
    "DATABASE = 'TEST'\n",
    "SCHEMA = 'PUBLIC'\n",
    "PASSWORD = 'Limetree123'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk upload to a table from a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below block of code will access the specified directory and traverse through each folder in the directory. It will then pick each folder and load it as a separate table in snowflake if the file not been already loaded. The data within each file will be read into a dataframe. We then create a connection to snowflake using sqlalchemy. The data from the dataframe is then appended into the specified table if the table already exists in snowflake else it will create a new table.\n",
    "\n",
    "All the files that error out will be recorded in the logs.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(URL(\n",
    "        user=USER,\n",
    "      password=PASSWORD,\n",
    "      account=ACCOUNT,\n",
    "      database=DATABASE,\n",
    "      schema=SCHEMA,\n",
    "      warehouse = WAREHOUSE\n",
    "    ))\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.execute('USE Schema TEST.public')\n",
    "connection.execute('USE warehouse COMPUTE_WH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING : Run this next block of code only if the file is being run for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x263e4a91128>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection.execute(''' CREATE or REPLACE TABLE \n",
    "  loaded_files\n",
    "    ( FILE_NAME varchar,\n",
    "    TABLE_NAME varchar)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['additional_noninterest_income', 'dep_basedon_100krepthreshold', 'small_business_loans', 'unused_commitments_sec']\n"
     ]
    }
   ],
   "source": [
    "loaded_files_df = pd.read_sql_query(\"SELECT * FROM LOADED_FILES\", engine)\n",
    "folders_directory = 'D:\\\\Coursework\\\\Capstone\\\\test'\n",
    "folders = os.listdir(folders_directory)# Change folder name\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs.txt\", \"a\") as myfile:\n",
    "    myfile.write(str(datetime.datetime.now())+\"\\n\")\n",
    "    for folder in folders:\n",
    "        files = os.listdir(folders_directory + '\\\\' + folder)\n",
    "        for file in files:\n",
    "            print('file:',file)\n",
    "            if file not in loaded_files_df.file_name.tolist():\n",
    "                try:\n",
    "                    current_file = {'table_name': [folder], 'file_name' : [file]}\n",
    "                    current_file_df = DataFrame(current_file,columns= ['table_name', 'file_name'])\n",
    "                    data = pd.read_csv(folders_directory + '\\\\' + folder + '\\\\' + file, encoding='iso-8859-1')\n",
    "                    data.to_sql(folder, con=connection, index=False, chunksize=200, if_exists='append') #make sure index is False, Snowflake doesnt accept indexes\n",
    "                    current_file_df.to_sql('loaded_files', con=connection, index=False, chunksize=200, if_exists='append')\n",
    "                except:\n",
    "                    myfile.write(file+\"\\n\")\n",
    "                    myfile.write(str(sys.exc_info())+\"\\n\")\n",
    "                    print(\"Oops!\",sys.exc_info(),\"occured.\")\n",
    "                    print(\"Next entry.\")\n",
    "                    print()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "connection.close()\n",
    "engine.dispose()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk upload from a file to a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code is very similar to the above one only difference is we will now only upload one file at a time from the specified path to the specified table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'D:\\\\Coursework\\\\Capstone'\n",
    "data = pd.read_csv(folder_path + file) \n",
    "engine = create_engine(URL(\n",
    "        user=USER,\n",
    "      password=PASSWORD,\n",
    "      account=ACCOUNT,\n",
    "      #warehouse=WAREHOUSE,\n",
    "      database=DATABASE,\n",
    "      schema=SCHEMA,\n",
    "      warehouse = 'compute_wh'\n",
    "))\n",
    "\n",
    "connection = engine.connect()\n",
    "#change table name\n",
    "data.to_sql('current_reporting_institutions', con=engine, index=False, chunksize=200,if_exists='append') #make sure index is False, Snowflake doesnt accept indexes\n",
    "\n",
    "connection.close()\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
